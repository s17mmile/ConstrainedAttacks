Write paper while all steps are ongoing.

Write an extensive README to maybe make the code in this project retain its uselfulness.


Keras previous version: 3.9.2 (newest?)
Tensorflow previous version: 2.16.1




1. ImageNet and MNIST: Save training and testing data separately.
    --> Decide exactly which data should be augmented. 
    --> I would assume we augment the training data while leaving training data unchanged.
    --> Exception for ImageNet, as I cannot find the training data: use one of the three test sets for augmentation, one for evaluation.

--> YOU ARE HERE
Update MNIST comparison script to show target and label instead of "old label" as it's inaccurate.
PGD: variable step size?
PGD: early stopping
Use del in more places to save memory
Make constrainers a local import!
Redo the "prediction success" outputs as the indexing can be wrong.
    --> Rename "success" arrays into something more appropriate (currently, it's 1 if the input is MISclassified, so 1-accuracy)
    --> Actually just get rid of the stupid success arrays. We have the labels, we can just check that in evaluation/display.


Introduce restrictions.
    --> For topodnn, find some shit that makes sense that isn't just Energy Conservation.
    --> Limit angular spread
    --> Introduce comparable restrictions for attacks on CIFAR-10 and ImageNet to see if their reactions are similar.
    --> Conclusion: does the same restriction applied to the same task produce comparable results?
    
Retrain Models. Evaluate Performance.

Make a nice readme.

Check comments of each attack function to make sure the details are correct.