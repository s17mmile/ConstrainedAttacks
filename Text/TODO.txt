Write evaluators:
    Write Feature and Label Histogram creator scripts
    Apply and measure different distance metrics
    Confusion Matrix (was previous classification correct vs. is perturbed classification correct) --> Fooling Ratio
    Correlation Matrix

Constrainer Architecture:
    --> First, Make constrainers and other useful maths stuff (e.g. linearRescale) a local import --> Helpers.constrainers or sth.
    --> Give constrainers both the original example and adversary candidate as parameters to enable more flexibility.

Build a proper constrainer for TopoDNN
    --> Read up on the physics that gets you the Energy
    --> Energy conservation. Also apply this as feasibilityProjector.
    --> Check out impact of nonexistent constituents --> if a constit is all 0 (doesn't exist), then the algorithm can't create a new one from thin air. Also apply this as feasibilityProjector.

Retraining
    --> a) Should take a model path and dataset as parameters, also a way to chunk the data for testing at different amounts of retraining.
    --> b) Should specify architecture and then train using a combination of the OG dataset and (chuunks of various sizes of) the adversarial data.
    --> Test retraining from scratch [using b)] and continued training of existing model [using a)].



Other:
Write Dataset and Model Overviews.
Check comments of each attack function to make sure the details are correct.
RDSA discussion: could we implement it without the need for precalc'd histograms? Calculate continuity, then just pick a uniformly random sample from which we pick the value (via a memmap? access might be a bitch). 

FUTURE:
    Can PGD and RDSA be combined somehow? A physics-oriented gradient-based attack that reduces correlations and retains distributions would be damn near perfect.
    (Evaluate Performance impact caused by calculating adversaries one-by-one in a function call and not using some numpy array or tensorflow tensor fuckery to have it run several in a single function call in the context of the chosen data structure (array/tensor)...)
    Reduce input and output dataset size by using smaller floats or doing some downsampling. We really do not need 64-bit color depth. Using less fine-grained data might massively speed up our overall performance too.
    Constrainers and FeasibilityProjectors should have a more flexible architecture, such as passing in a "Constrainer" class/subclass instance to allow for the passing of more/variable parameters.